{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SDS PhD 2020 - Supervised ML  tutorial",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pOO1KFlyCIt"
      },
      "source": [
        "# A quick hands-on tutorial in Supervised ML\n",
        "\n",
        "In this tutorial we are going to use an already clean dataset from the [Nomadlist Cities](https://nomadlist.com/) data to predict the continent where the city is located.\n",
        "\n",
        "![alt text](https://source.unsplash.com/1JNk998-g70/800x600)\n",
        "\n",
        "We are going to encode target-label (world regions), scale our features and try out different algorithms including LogisticRegression, Random Forest and XGBoost. \n",
        "\n",
        "We also will try to predict the `nomad-score`, a continuous variable. This is a different kind of problem - a regression problem. We will need to use slightly different tooling for model-fitting and for evaluation.\n",
        "\n",
        "The tutorial will mostly rely on the Sklearn ML library.\n",
        "You will see that the syntax and logic of Sklearn is also used in other new libraries like XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzY4IQJCvFk9"
      },
      "source": [
        "# Import standard Libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import altair as alt\n",
        "\n",
        "\n",
        "sns.set(rc={'figure.figsize':(10,10)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozqlEmY_1X-I"
      },
      "source": [
        "## Loading and selecting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR_dyYa8XUif"
      },
      "source": [
        "# Load data\n",
        "data = pd.read_csv('https://github.com/CALDISS-AAU/sdsphd20/raw/master/datasets/cities_sds_phd.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_N9wWP3qDlAU"
      },
      "source": [
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwzfKUm2v6I6"
      },
      "source": [
        "# Select the (independant) features that we are going to use to train the model\n",
        "X = data.loc[:,'cost_nomad':'weed']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quFteZ5SwSzW"
      },
      "source": [
        "# Define the dependant variabel / target to predict (world region)\n",
        "y = data.region"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moDGB3-i1dx3"
      },
      "source": [
        "## Transforming, preprocessing and splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpeSJSkvwp0-"
      },
      "source": [
        "# Load and instantiate a LabelEncoder that will turn our text labels (regions into indices)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "encoder = LabelEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NanA9OsOxHVt"
      },
      "source": [
        "# Transform labels into indices by passing y to the encoder\n",
        "y_enc = encoder.fit_transform(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhyluncNxVJ6"
      },
      "source": [
        "# Load and instantiate a StandardSclaer \n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diIu55Vzxta_"
      },
      "source": [
        "# Apply the scaler to our X-features\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrL3QRCxyBVN"
      },
      "source": [
        "# Split the data using the train_test_split module. We keep 20% of the data for testing and use 80% to train the model\n",
        "# Random state defined with an arbitrary number for reproducibility\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_enc, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2KCzi2o1iyw"
      },
      "source": [
        "## Training and evaluating various models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MkQFoNe1vmT"
      },
      "source": [
        "# Import modules that we are going to use for all models\n",
        "\n",
        "# Import K-fold crossvalidation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Import Classification Report for later evaluatoion of performance\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLFEsvms1_2S"
      },
      "source": [
        "### LogisticRegression (let's call it that for now without going into details)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXExQ2Kvy4z2"
      },
      "source": [
        "# Import and instantiate the model\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(max_iter=2000)\n",
        "\n",
        "# K-fold cross-validation (splitting the 80% into 5 chunks, using 4 to train and 1 to evaluate)\n",
        "scores = cross_val_score(model, X_train, y_train, cv = 5)\n",
        "print(scores)\n",
        "\n",
        "# Model training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model performance on the test-set\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gibHMkps8rV0"
      },
      "source": [
        "The overall accuracy is at 65% which is not too impressive. In a multiclass setting that number is also somewhat hard to interpret and that's where it's useful to look at other evaluation statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btJR6zlS2pjJ"
      },
      "source": [
        "# Performance evaluation using the classification_report\n",
        "\n",
        "target_names = encoder.inverse_transform(list(set(y_test))) # get real region names back using inverse_transform\n",
        "\n",
        "y_pred = model.predict(X_test) # predict from the testset\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names = target_names)) #Print out the report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNAbnJtL3SdX"
      },
      "source": [
        "Logistic regression is not doing too well. It is particularly bad when predicting African cities.\n",
        "\n",
        "Here the recall score is perhaps more interesting than the precision score.\n",
        "\n",
        "#### Let's inspect the performance visually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UW4cJO2DLs4"
      },
      "source": [
        "!!pip uninstall -qq mlxtend -y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYhRTfaE6Rz9"
      },
      "source": [
        "# For that we need to install an updated version of the MLxtend library (it will make plotting of the confusion matrix easy)\n",
        "!pip install -qq -U mlxtend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l8tLiTa3dhd"
      },
      "source": [
        "# Import the confusion matrix plotter module\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "# We will also import sklearns confusion matrix module that will make it easy to produce a confusion matrix\n",
        "# It's actually just a cross-tab of predicted vs. real values\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jY11XzX4NlH"
      },
      "source": [
        "# calculate the confusion matrix\n",
        "confmatrix = confusion_matrix(y_test,y_pred) \n",
        "\n",
        "# Let's plot\n",
        "plot_confusion_matrix(conf_mat=confmatrix,\n",
        "                                colorbar=True,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                hide_spines = True,\n",
        "                                class_names=target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJvCdog_86Z1"
      },
      "source": [
        "As you can see, the model struggeled a lot with African cities and places in Oceania. That is probably also because those are not too many in the data and thus it is hard for the model to learn abot their characteristics.\n",
        "\n",
        "Some vities form the Americas have been placed in Europe (probably places like Boston or cities in Latinamerica that are similar to Southern Europe). It's an interesting exercise to explore misplaced observations..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx7O9HX18iZJ"
      },
      "source": [
        "### Random Forest\n",
        "Now we can try out a more complex model (and hopefully more powerfull)\n",
        "The process is exactly the same and thus there are not too many comments in the code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMiwIBac0IIx"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv = 5)\n",
        "print(scores)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKicrTyE93iN"
      },
      "source": [
        "The test-score is well within the values produced in the crossvalidation\n",
        "Overall performance goes up (as expected)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9Ce060X-FOU"
      },
      "source": [
        "# Performance evaluation using the classification_report\n",
        "\n",
        "target_names = encoder.inverse_transform(list(set(y_test))) # get real region names back using inverse_transform\n",
        "\n",
        "y_pred = model.predict(X_test) # predict from the testset\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names)) #Print out the report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2imM0GCy-bTi"
      },
      "source": [
        "# calculate the confusion matrix\n",
        "confmatrix = confusion_matrix(y_test,y_pred) \n",
        "\n",
        "# Let's plot\n",
        "plot_confusion_matrix(conf_mat=confmatrix,\n",
        "                                colorbar=True,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                class_names = target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wHrBf9k-Kqv"
      },
      "source": [
        "While the model is better at classification of the lartger groups, performance is the same for Oceania and goes down for Africa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R9bYtXwAiop"
      },
      "source": [
        "### XGBoost\n",
        "Finally, XGBoost (again we will use standard settings - i.e. no hyperameter tuning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFFNCzC1bX6"
      },
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "model = xgb.XGBClassifier()\n",
        "\n",
        "scores = cross_val_score(model, X_train, y_train, cv = 5)\n",
        "print(scores)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjFtXk9DAz48"
      },
      "source": [
        "Overall performance is even higher as with Catboost. But let's see how the algorighm is dealing with our problematic small classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m12vHNqyLg9b"
      },
      "source": [
        "# Performance evaluation using the classification_report\n",
        "\n",
        "target_names = encoder.inverse_transform(list(set(y_test))) # get real region names back using inverse_transform\n",
        "\n",
        "y_pred = model.predict(X_test) # predict from the testset\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=target_names)) #Print out the report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBzruzJKA84i"
      },
      "source": [
        "# calculate the confusion matrix\n",
        "confmatrix = confusion_matrix(y_test,y_pred) \n",
        "\n",
        "# Let's plot\n",
        "plot_confusion_matrix(conf_mat=confmatrix,\n",
        "                                colorbar=True,\n",
        "                                show_absolute=True,\n",
        "                                show_normed=True,\n",
        "                                class_names = target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRahPtwYBHKL"
      },
      "source": [
        "Overall, it seems XGBoost wins this time.\n",
        "\n",
        "This notebook is only a quick example of the machanics of valious algorithms on small data. \n",
        "In real-world situations we would need to spend much more time tuning the models. Also: More compelx models do not always perform better..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5I_3lXhonnRS"
      },
      "source": [
        "## Predicting the Nomad Score\n",
        "\n",
        "So far we have considered a classificaion problem - the model had to pick one of the 5 options. The outcome variable was a class. Let's shift gears and look at a different type of problem - a prediction where the outcome is a continuous variable. This is our \"typical\" regression problem.\n",
        "\n",
        "In the following we are going to predict the nomad score. The inputs into the model will be the same that we already used for predicting the region. We are only going to change the dependant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CShzjDg6oob6"
      },
      "source": [
        "# picking a different outcome variable\n",
        "\n",
        "y_reg = data.nomad_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7m05yvrotXX"
      },
      "source": [
        "# We need to creat new train / test splits here - as the nomad_score was not part of the previous split.\n",
        "\n",
        "X_train, X_test, y_train, y_test, data_train, data_test = train_test_split(X_scaled, y_reg, data, test_size = 0.2, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEVybQgQ1Oh2"
      },
      "source": [
        "as you can see, I also have the overall dataframe in the split as a 3rd coponent. This is only for some interactive visuals down the line. But yeah, you can do that too... :-) Sometimes it's also handy when passing in some indices that you want to use to get back to data that would be inaccessible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IGuRAojpY5B"
      },
      "source": [
        "# Import and instantiate the baseline model\n",
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "\n",
        "# Model training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model performance on the test-set / This score is not accacy but a R^2\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsOJX2FYqNIO"
      },
      "source": [
        "# We can also inspect our results visually\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "sns.scatterplot(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWBzfSIhzv5l"
      },
      "source": [
        "data_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BklkEULzyQI7"
      },
      "source": [
        "data_test['nomad_score_pred'] = y_pred\n",
        "\n",
        "alt.Chart(data_test).mark_circle(size=60).encode(\n",
        "    x='nomad_score',\n",
        "    y='nomad_score_pred',\n",
        "    color=alt.Color('region', scale=alt.Scale(scheme='category10')),\n",
        "    tooltip=['region','weed','place']\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8Itd6A4qfqP"
      },
      "source": [
        "Let's try a different model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zJyQOtAqU97"
      },
      "source": [
        "# Import and instantiate a XGBoost Regressor\n",
        "\n",
        "model = xgb.XGBRegressor()\n",
        "\n",
        "# Model training\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Model performance on the test-set / This score is not accacy but a R^2\n",
        "print(model.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouapfZxaq5tC"
      },
      "source": [
        "# We can also inspect our results visually\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "sns.scatterplot(y_test,y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nYmErBaAYsY"
      },
      "source": [
        "data_test['nomad_score_pred'] = y_pred\n",
        "\n",
        "alt.Chart(data_test).mark_circle(size=60).encode(\n",
        "    x='nomad_score',\n",
        "    y='nomad_score_pred',\n",
        "    color=alt.Color('region', scale=alt.Scale(scheme='category10')),\n",
        "    tooltip=['region','weed','place']\n",
        ").interactive()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzsdiWJfuB3q"
      },
      "source": [
        "# Your turn\n",
        "\n",
        "\n",
        "In the repo, you will find a dataset describing employee turnover in a company.\n",
        "\n",
        "https://raw.githubusercontent.com/CALDISS-AAU/sdsphd20/master/datasets/turnover.csv\n",
        "\n",
        "The dataset contains data collected in an employee survey and enriched with HR data.\n",
        "\n",
        "The variable `churn` tells us if the employee left the company in the past 3 months. The other variables are collected\n",
        "\n",
        "## Classification\n",
        "\n",
        "Try to predict `churn` using a classification pipeline (perhaps add some simple exploration of the data first)\n",
        "\n",
        "## Regression\n",
        "Try to predict the number of weekly average hours worked.\n",
        "\n",
        "**Before** working with the data, you should use `pd.get_dummies` to get dummies for categorical variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlcnk9NNFVR-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}